---
title: "Follow-up Problem"
description: |
  Solution to Lambda Capital's follow-up problem
author:
  - name: Alexis Solis Cancino
    url: mailto:alexis.solisc@gmail.com
    # affiliation: ITAM
date: "`r Sys.Date()`"
output: 
  distill::distill_article:
    toc: true
    toc_float: true
    code_folding: true
    theme: "resources/theme.css"
editor_options: 
  chunk_output_type: console
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T)
library(tidyverse)
library(lubridate)
library(glue)
library(skimr)
```


### Foreword on Reproducibility

In order to run this notebook, you need to have [R](https://cran.r-project.org/) installed, ideally [RStudio](https://rstudio.com/products/rstudio/download/#download) too. Also, you would need to install the following packages:

```{r, eval=FALSE}
# NOT RUN
# install.packages(c("tidyverse", "lubridate", "glue", "skimr"))
```


## 1. Introduction

The task is to develop a backtest for a strategy that trades constituents of the S&P 500 index from January 1st, 2010 through January 1st, 2020. Each day the strategy selects the 10 stocks with the highest [Price Rate of Change Indicator (ROC)](https://www.investopedia.com/terms/p/pricerateofchange.asp) and buys them on an equally-weighted basis.

It's important to verify that it only selects stocks that are part of the S&P 500 index at each moment in time, as these change every quarter. A file should be generated with the values of the equity curve (Net Asset Value) over time and a file of weights or positions where you can see the stocks that were selected each day.


### 1.1 The S&P 500 Index

The S&P 500 index is one of the most-widely known indexes in finance. According to S&P Dow Jones Indices: 'the S&P 500® is widely regarded as the best single gauge of large-cap U.S. equities. There is over USD 11.2 trillion indexed or benchmarked to the index, with indexed assets comprising approximately USD 4.6 trillion of this total. The index includes 500 leading companies and covers approximately 80% of available market capitalization.'

The index measures the performance of the large-cap segment of the market. Considered to be a proxy of the U.S. equity market, the index is composed of 500 constituent companies.


According to the [S&P 500's methodology](https://www.spglobal.com/spdji/en/documents/methodologies/methodology-sp-us-indices.pdf), the index is weighted by float-adjusted market capitalization and is rebalanced on a quarterly basis.

### 1.2 The Strategy

The strategy consists of selecting the 10 stocks with the highest [Price Rate of Change Indicator (ROC)](https://www.investopedia.com/terms/p/pricerateofchange.asp) and buys them on an equally-weighted basis. Each day before the market opens, we look back on the Price ROCs of the previous close and buy the 10 stocks with the highest ROC.

### 1.3 The Price Rate of Change (ROC)

The Price ROC for 200 days is calculated as:

\begin{equation}
\text{ROC}_{200d} = \left(\dfrac{P_t}{P_{t-200}}-1\right) \times 100
\end{equation}


## 2. Constituents come first

We were provided with two datasets: `closes.csv` and `constituents.csv`. We will begin by exploring the latter.

### 2.1 Constituents Data EDA


We import the `constituents.csv` data.

```{r}
constituents <- read_csv("data/constituents.csv", col_types = "dDcccccc") %>% 
  # Remove first column, which has the index
  select(-1)

constituents
```


Let's use the `skimr` package to print a quick summary of missing and unique values of the data:

```{r}
constituents %>% 
  skimr::skim()
```

Some notes on the table above:

1. The dataset consists of 6 character-type variables and 1 date-type variable.

2. According to the table above, there are 4 unique values for the `action` variable. Let's see what are those:

```{r}
constituents %>% distinct(action)
```

3. The data goes from `r min(constituents$date)` through `r max(constituents$date)` but we don't have daily data since there are only `r unique(constituents$date) %>% length()` unique dates.


4.  The columns `contraticker`, `contraname` and `note` are the only columns that have missing values, which is both logical and acceptable.

5. The `contraticker` and `contraname` columns see to be the opposite of the stocks that are added/removed to the index. Let's check that with some steps:

* Check all possible different `action` values when there are no missing values of either the `contraticker` or `contraname` variables.

```{r}
constituents %>% 
  filter(!is.na(contraticker) | !is.na(contraname)) %>% 
  distinct(action)
```

* Inspect some rows where there are no missing values of either the `contraticker` or `contraname` variables.

```{r}
constituents %>% 
  filter(!is.na(contraticker) | !is.na(contraname)) 
```

Indeed this is the case. We can see, for example, on the first row: TSLA is being added and replaced by AIV. On the next row we see the opposite operation: AIV is being removed and substituted by TSLA.


* Another important check we must do is that there are no errors in the ticker-name relationship. That is, for every (ticker, name) pair there must be just one count per value-key pair.

We first get the count for each key-value pair:

```{r}
constituents %>% distinct(ticker, name) %>% count(ticker, name, name = "count")
```

Now we filter the counts that are greater than 1:

```{r}
constituents %>% 
  distinct(ticker, name) %>% 
  count(ticker, name, name = "count") %>% 
  filter(count > 1)
```

We get no rows, which means that the data is correct.


### 2.1.1 Time variables and `action` in the `constituents` dataset

In this section we'll create a time series that will contain, for each date, the constituents of the S&P 500 index.

Since we are focusing on time, we'll add some time-related variables, such as:

-   `year`: The year of the *time series* `date`.
-   `half`: The *half component* of the index (i.e. to which semester does the date belong to).
-   `quarter`: The *quarter component* of the index (i.e. to which quarter does the date belong to).
-   `month`: The *month component* of the index (with base 1 - that is, January = 1 and so on).
-   `month_label`: The three-letter month label as an ordered categorical variable. It begins with *Jan* and ends with *Dec*.
-   `day`: The *day* component of the `date`.
-   `wday`: The day of the week with base 1. Monday = 1 and Sunday = 7.
-   `wday_label`: The three-letter label for day of the week as an ordered categorical variable. It begins with `Mon` and ends with `Sun`.
-   `qday`: The day of the quarter.
-   `yday`: The day of the year.
-   `qid`: The quarter's ID.

All variables will have the `date_` prefix so they are easy to identify. Since we can use this creation of variables for later, we create the `add_ts_signature()` function (so we can _recycle_ the code).

```{r}
add_ts_signature <- function(df, .date_col) {
  df %>% 
    # Create time series signature
    mutate(
      date_year        = year({{.date_col}}),
      date_half        = semester({{.date_col}}, with_year = FALSE),
      date_quarter     = quarter({{.date_col}}),
      date_month       = month({{.date_col}}),
      date_month_label = month({{.date_col}}, label = TRUE),
      date_day         = day({{.date_col}}),
      date_wday        = wday({{.date_col}}, week_start = 1),
      date_wday_label  = wday({{.date_col}}, label = TRUE),
      date_qday        = qday({{.date_col}}),
      date_yday        = yday({{.date_col}}),
      date_qid         = str_c(date_quarter,"Q", str_sub(date_year, 3L)) %>% as_factor()
    )
}
```

```{r}
constituents_ts <- constituents %>% 
  add_ts_signature(.date_col = date)
```


How are `actions` distributed along the data?

```{r}
constituents_ts %>% count(action, sort = T)
```

`current` appears to correspond to the index's most recent constituents. We can check if this is the case by taking the unique values for the `date` variable when the `action` variable has the _current_ value.

```{r}
constituents_ts %>% 
  filter(
    action == "current"
  ) %>% 
  distinct(date)
```

Indeed, the only date where a _current_ value appears is `r constituents_ts %>% filter(action == "current") %>% distinct(date)`


Let's inspect the data for the first date `1957-03-04`:

```{r}
constituents_wide %>% 
  filter(
    date == ymd("1957-03-04")
  )
```

It's clear that we don't have the initial S&P 500 sample, but we **could** reverse engineer the constituents at each date. Let's start by the final date `2020-12-31`:

```{r}
constituents %>% 
  filter(date == ymd("2020-12-31"))
```


We see that we have sort-of-duplicated rows; we note that one `action` label corresponds to `historical` and the other one to `current`. Let's check that we actually have the same number per category:

```{r}
constituents %>% 
  filter(date == ymd("2020-12-31")) %>% 
  count(action)
```

In order to get a better sense of what's going on with the `action` column, we pivot the data to a _wide_ format so we can clearly see the difference between each `action` value. By doing this, we can have a single column for each of the 4 unique values in the `action` variable.

```{r}
constituents_wide <- constituents_ts %>% 
  pivot_wider(
    names_from = action,
    values_from = name
  ) %>% 
  relocate(historical, current, added, removed, .after = ticker)

constituents_wide
```

Inspecting the data, it appears that the `historical` variable gives us the constituents at each quarter. This is backed by the fact that on the latest available date, we have the `current` constituents which correspond to the 4Q20. However, as mentioned above, we have the exact same rows but with a `historical` label.

Let's inspect the dates on which there are no missing values for the `historical` action. We can check the unique values for the month number:

```{r}
constituents_wide %>% 
  filter(!is.na(historical)) %>%
  distinct(date_month, date_quarter)
```


Indeed, the only months were `historical` actions are not missing are _mod 3_ months (i.e. the end of each quarter).

This points to the fact that probably the `historical` value indicates the constituents per quarter on the S&P 500. We can further check the amount of `historical` values per quarter. The number should be around 500, of course.

```{r}
constituents_wide %>% 
  select(date_year, date_qid, historical) %>% 
  filter(date_year > 2008, !is.na(historical)) %>% 
  group_by(date_year, date_qid) %>%  
  nest() %>% 
  mutate(constituent_count = map_int(data, nrow)) %>% 
  select(-data) %>% 
  print(n = 45)
```


Indeed the number is very close to 500 among all quarters.

One last check we can perform before we can use the `historical` rows as our constituents has to do with the `current` rows. We can reverse engineer the 3Q20 constituents by starting from the 4Q20 (the _current_ constituents).


We can first see what the changes on constituents were between the 3Q20 and 4Q20. We can separate the adds and drops into two data frames, like so:

```{r}
adds_drops <- constituents_wide %>% 
  filter(!is.na(added) | !is.na(removed) , date_qid %in% c("4Q20")) %>% 
  select(date, ticker, added, removed, date_qid)

adds_drops %>% 
  filter(!is.na(added)) %>% 
  select(-removed)
```

So TSLA and VNT were added in the 4Q20.

```{r}
adds_drops %>% 
  filter(!is.na(removed)) %>% 
  select(-added)
```

NBL and AIV were removed in the 4Q20. Apparently, substituted by TSLA and VNT.


We can now do the reverse action for the adds and the drops, respectively. Thus, the drops from 3Q20-4Q20 will become our adds and the adds will become our drops.

We start by getting the adds needed to go from 4Q20 to 3Q20:

```{r}
# Extract the adds from 4Q20 to 3Q20
adds_3Q <- adds_drops %>% 
  filter(!is.na(removed)) %>% 
  pull(removed)

adds_3Q
```

Next, we get the drops needed to go from 4Q20 to 3Q20:

```{r}
# Extract the drops from 4Q20 to 3Q20
drops_3Q <- adds_drops %>% 
  filter(!is.na(added)) %>% 
  pull(added)

drops_3Q
```


Finally, we get the constituents in 4Q20, include the adds and remove the drops, and compare against the constituents in 3Q20. We should get the same result.

```{r}
# Get all the constituents for 4Q20
constituents_4Q20 <- constituents_wide %>% 
  filter(date_qid == "4Q20", !is.na(historical)) %>% 
  pull(historical) %>% sort()

# Get all the constituents for 3Q20
constituents_3Q20 <- constituents_wide %>% 
  filter(date_qid == "3Q20", !is.na(historical)) %>% 
  pull(historical) %>% sort()

# include the adds from 4Q20
calculated_3Q20 <- c(constituents_4Q20, adds_3Q)

# remove the drops from 4Q20
calculated_3Q20 <- calculated_3Q20[!calculated_3Q20 %in% drops_3Q] %>% sort()

# Check if both results are equal
all.equal(constituents_3Q20, calculated_3Q20)
```

Voilà! We did get a `TRUE` value when comparing the reverse-engineered approach and the mere `historical` values. This confirms that we can use the `historical` values for our constituents.

### 2.1.2 The time series of S&P 500 constituents

We will now proceed to use these `historical` values for every day in the `closes.csv` dataset from `2010-01-01` through `2020-01-01`.


```{r}
# Get the required dates from the closes.csv file
required_dates <- read_csv(file = "data/closes.csv") %>% 
  # Focus on the date only
  select(date) %>% 
  
  # Focus on 2010 or later
  filter(year(date) > 2008)
```

We now nest a data frame for each date in the `constituents` dataset that contains the S&P 500 constituents.

```{r}
constituents_nested <- constituents_wide %>% 
  # Focus on just the historicals and 2010 or later
  filter(date_year > 2008, !is.na(historical)) %>% 
  
  # Just take the date, ticker, and historical columns
  select(date, ticker, historical) %>% 
  
  # Nest the data
  nest(constituents = c(ticker, historical)) %>% 
  relocate(constituents, .after = 1) %>% 
  arrange(date) %>% 
  
  # Uncomment in case we want to get the number of constituents per date
  # mutate(constituent_count = map_int(constituents, nrow)) %>% 
  identity()

constituents_nested
```

We can now left_join the datasets:

```{r}
constituents_nested <- required_dates %>% 
  left_join(constituents_nested, by = "date") %>% 
  fill(contains("constituent"), .direction = "up")

constituents_nested
```


Finally! We now have a time series with the corresponding composition of the S&P 500 for each date.


## 3. Implementing the ROC Strategy

We proceed in the following way:

First, import the data:

```{r}
# Use read_csv() function to read the close data
close_tbl <- read_csv("data/closes.csv") %>% 
  select(-1)

close_tbl
```

Apparently, we have daily closing data from `1997-12-31` through `2021-01-12` for `1,084` different securities.

### 3.1 Calculating the 200-day ROC

Secondly, calculate the 200-day ROC. To achieve this, we create the `add_price_roc()` function, which takes an `ndays` argument to indicate the number of days for the ROC calculation. This would be useful if we'd like to explore different days for the ROC.

```{r}
# This functions needs a data frame that has a date column and the rest of the columns must have the price of the securities.
add_price_roc <- function(df, .date_col, .ndays) {
  
  df %>% 
    mutate(
      across(
        .cols = -{{.date_col}},
        .fns = ~ ((.x / lag(.x, n = .ndays)) - 1) * 100,
        .names = "ROC_{.col}"
      )
    ) %>% 
    # keep rows where at least one of the ROC columns is not NA
    filter(
      if_any(contains("ROC"), ~ !is.na(.x))
    )
}
```

Now let's use this function on our `closes.csv` data. This creates a column for each security that contains the `n-day` ROC for each security. The ROC numbers are in columns with the _ROC_ prefix to easily identify them.

```{r}
roc_close <- close_tbl %>% 
  # Add 200-day ROC
  add_price_roc(.date_col = date, 
                .ndays = 200) %>%
  relocate(date, contains("ROC"))

roc_close
```

### 3.2 Filtering data from 2010 onward

Since the strategy will be implemented from 2010 onward, we can now filter out the data for previous years:

```{r}
roc_close <- roc_close %>% 
  filter(year(date) > 2008)
```

### 3.3 Getting the data into a 'long' format

We get the data into a long format (instead of the wide format with `2,169` columns). Also, a separation is made in two data frames: one for ROCs and the other one for prices. We call these `roc_long` and `close_long`, respectively.


```{r}
# Build a dataframe with ROC values
roc_long <- roc_close %>% 
  select(date, contains("ROC")) %>% 
  pivot_longer(
    cols = contains("ROC"),
    names_to = "ticker",
    names_pattern = "_(.*)",
    values_to = "roc"
  ) %>% 
  add_column(data_type = "ROC")

roc_long
```


```{r}
# Build a dataframe with price values
close_long <- roc_close %>% 
  select(-contains("ROC")) %>% 
  pivot_longer(
    cols = -date,
    names_to = "ticker",
    values_to = "price"
  ) %>% 
  add_column(data_type = "close")

close_long
```

We now nest both dataframes so we have a similar structure to the `constituents` data.

```{r}
roc_nested <- roc_long %>% 
  nest(rocs = c(ticker, roc, data_type))
  
roc_nested
```

```{r}
close_nested <- close_long %>% 
  nest(prices = c(ticker, price, data_type))

close_nested
```

### 3.4 Consolidated, Nested Data

Having all three nested dataframes (constituents, prices and rocs) we can consolidate everything into a single dataframe. This will be used to analyze the strategy.

```{r}
consolidated_nested <- constituents_nested %>% 
  left_join(close_nested) %>% 
  left_join(roc_nested) %>% 
  mutate(row_id = row_number()) %>% 
  relocate(row_id, everything())

consolidated_nested
```

In this `consolidated_nested` dataframe each row represents a single date. The advantage of this dataframe is that it contains the constituents, the prices, and rocs for each date.


### 3.5 Stock Selection Per Day

To code the stock selection, we start by defining the starting and ending dates.

```{r}
start_date <- ymd("2010-01-01")
end_date <- ymd("2020-01-01")
```

Then, we can filter the `consolidated_nested` dataframe so we have data only for this range of dates.

```{r}
consolidated_nested <- consolidated_nested %>% 
  filter(date %>% between(start_date - 1, end_date + 1)
  )
```


We then execute the following steps:

1. For each day, get the constituents of the S&P 500.

For this purpose, we create the `extract_constituents()` function:

```{r}
extract_constituents <- function(df, extraction_date) {
  df %>% 
    filter(date == extraction_date) %>% 
    select(row_id, date, constituents) %>% 
    unnest(cols = constituents) %>% 
    pull(ticker)
}
```

We can test it for any date and show just the first 6 tickers:

```{r}
extract_constituents(consolidated_nested, ymd("2019-12-31")) %>% 
  head()
```


2. Then, rank each of those constituents by their previous closing 200-day ROC. Choose the top 10 stocks.

For this purpose, we create two functions:

- the `show_top10_roc()` function, which shows us the top 10 tickers ranked by ROC, for any given date.

- the `extract_top10_roc()` function, which extracts the top 10 tickers ranked by ROC, for any given date.

Here's the `show_top10_roc()` function:

```{r}
show_top10_roc <- function(df, extraction_date) {
  
  # Get the ID for the previous close
  previous_row <- df %>% 
    filter(date == extraction_date) %>% 
    pull(row_id) - 1
  
  # Proceed to get the top 10 tickers
  df %>% 
    filter(row_id == previous_row) %>% 
    select(row_id, date, rocs) %>% 
    unnest(cols = rocs) %>% 
    
    # Filter to focus on the S&P 500 constituents. Use extract_constituents function
    filter(ticker %in% extract_constituents(df, extraction_date)) %>% 
    
    # Get the top 10, as ordered by ROC
    slice_max(order_by = roc, n = 10)
  
}
```

Let's test this function for a given date:

```{r}
show_top10_roc(consolidated_nested, ymd("2019-12-31"))
```

Indeed it shows the top 10 ROC tickers for the previous close.

Now here's the `extract_top10_roc()` function:

```{r}
extract_top10_roc <- function(df, extraction_date) {
  
  show_top10_roc(df, extraction_date) %>% 
    pull(ticker)
}
```

Again, we take it for a spin:

```{r}
extract_top10_roc(consolidated_nested, ymd("2019-12-31"))
```


It works perfectly; we got the exact same tickers as with the `show_top10_roc()` function.


3. Get the corresponding weights for each stock.

```{r}
chosen_tickers <- extract_top10_roc(consolidated_nested, ymd("2019-12-31"))

tibble(
  date = ymd("2019-12-31"),
  tickers = extract_constituents(consolidated_nested, ymd("2019-12-31")),
  weights = if_else(
    tickers %in% chosen_tickers, 
    1 / length(chosen_tickers), # equally weighted portfolio
    0                           # if ticker is not chose, assign 0 weight
  )
) %>% 
  arrange(-weights) %>% 
  print(n = 20)
```









# 4. Strategy Assumptions

The execution of this strategy, as presented on this solution, relies upon the following assumptions:

1. No trading friction (i.e. there are no commissions and taxes).
2. The closing price of the previous day is equal to the opening price of the current day.
3. The investor can buy fractions of shares.


The code can be further improved to 'relax' all of the previous assumptions and reflect a 'more realistic' implementation.

Delivery:

```{r}
tibble(
  date = ymd("2010-01-01"),
  nav = 1e6
)
```

```{r}
tibble(
  date = ymd("2010-01-01"),
  ticker = "AAPL",
  weight = .5
)
```


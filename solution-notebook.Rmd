---
title: "Follow-up Problem"
description: |
  Solution to Lambda Capital's follow-up problem
author:
  - name: Alexis Solis Cancino
    url: mailto:alexis.solisc@gmail.com
    # affiliation: ITAM
date: "`r Sys.Date()`"
output: 
  distill::distill_article:
    toc: true
    toc_float: true
    code_folding: true
    theme: "resources/theme.css"
editor_options: 
  chunk_output_type: console
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T)
library(tidyverse)
library(lubridate)
library(glue)
library(skimr)
```


### Foreword on Reproducibility

In order to run this notebook, you need to have [R](https://cran.r-project.org/) installed, ideally [RStudio](https://rstudio.com/products/rstudio/download/#download) too. Also, you would need to install the following packages:

```{r, eval=FALSE}
# NOT RUN
# install.packages(c("tidyverse", "lubridate", "glue", "skimr"))
```


## 1. Introduction

The task is to develop a backtest for a strategy that trades constituents of the S&P500 index from January 1st, 2010 through January 1st, 2020. Each day the strategy selects the 10 stocks with the highest [Price Rate of Change Indicator (ROC)](https://www.investopedia.com/terms/p/pricerateofchange.asp) and buys them on an equally-weighted basis.

It's important to verify that it only selects stocks that are part of the S&P500 index at each moment in time, as these change every quarter. A file should be generated with the values of the equity curve (Net Asset Value) over time and a file of weights or positions where you can see the stocks that were selected each day.


### 1.1 The S&P500 Index

The S&P500 index...

it rebalances every...

### 1.2 The Strategy

The strategy consists of selecting the 10 stocks with the highest [Price Rate of Change Indicator (ROC)](https://www.investopedia.com/terms/p/pricerateofchange.asp) and buys them on an equally-weighted basis. Each day before the market opens, we look back on the Price ROCs of the previous close and buy the 10 stocks with the highest ROC.

### 1.3 The Price Rate of Change (ROC)

The Price ROC for 200 days is calculated as:

\begin{equation}
\text{ROC}_{200d} = \left(\dfrac{P_t}{P_{t-200}}-1\right) \times 100
\end{equation}


## 2. Data comes first

We were provided with two datasets: `closes.csv` and `constituents.csv`. We will begin by exploring the latter.

### 2.1 Constituents Data EDA


We import the `constituents.csv` data.

```{r}
constituents <- read_csv("data/constituents.csv", col_types = "dDcccccc") %>% 
  # Remove first column, which has the index
  select(-1)

constituents
```


Let's use the `skimr` package to print a quick summary of missing and unique values of the data:

```{r}
constituents %>% 
  skimr::skim()
```

Some notes on the table above:

1. The dataset consists of 6 character-type variables and 1 date-type variable.

2. According to the table above, there are 4 unique values for the `action` variable. Let's see what are those:

```{r}
constituents %>% distinct(action)
```

3. The data goes from `r min(constituents$date)` through `r max(constituents$date)` but we don't have daily data since there are only `r unique(constituents$date) %>% length()` unique dates.


4.  The columns `contraticker`, `contraname` and `note` are the only columns that have missing values, which is both logical and acceptable.

5. The `contraticker` and `contraname` columns see to be the opposite of the stocks that are added/removed to the index. Let's check that with some steps:

* Check all possible different `action` values when there are no missing values of either the `contraticker` or `contraname` variables.

```{r}
constituents %>% 
  filter(!is.na(contraticker) | !is.na(contraname)) %>% 
  distinct(action)
```

* Inspect some rows where there are no missing values of either the `contraticker` or `contraname` variables.

```{r}
constituents %>% 
  filter(!is.na(contraticker) | !is.na(contraname)) 
```

Indeed this is the case. We can see, for example, on the first row: TSLA is being added and replaced by AIV. On the next row we see the opposite operation: AIV is being removed and substituted by TSLA.


* Another important check we must do is that there are no errors in the ticker-name relationship. That is, for every (ticker, name) pair there must be just one count per value-key pair.

We first get the count for each key-value pair:

```{r}
constituents %>% distinct(ticker, name) %>% count(ticker, name, name = "count")
```

Now we filter the counts that are greater than 1:

```{r}
constituents %>% 
  distinct(ticker, name) %>% 
  count(ticker, name, name = "count") %>% 
  filter(count > 1)
```

We get no rows, which means that the data is correct.


### 2.1.1 Time variables and `action` in the `constituents` dataset

In this section we'll create a time series that will contain, for each date, the constituents of the S&P500 index. We'll start by focusing on the `action` column.

Since we are focusing on time, we'll add some time-related variables, such as:

-   `year`: The year of the *time series* `date`.
-   `half`: The *half component* of the index (i.e. to which semester does the date belong to).
-   `quarter`: The *quarter component* of the index (i.e. to which quarter does the date belong to).
-   `month`: The *month component* of the index (with base 1 - that is, January = 1 and so on).
-   `month_label`: The three-letter month label as an ordered categorical variable. It begins with *Jan* and ends with *Dec*.
-   `day`: The *day* component of the `date`.
-   `wday`: The day of the week with base 1. Monday = 1 and Sunday = 7.
-   `wday_label`: The three-letter label for day of the week as an ordered categorical variable. It begins with `Mon` and ends with `Sun`.
-   `qday`: The day of the quarter.
-   `yday`: The day of the year.
-   `qid`: The quarter's ID.

All variables will have the `date_` prefix so they are easy to identify. Since we can use this creation of variables for later, we create the `add_ts_signature` function (so we can _recycle_ the code).

```{r}
add_ts_signature <- function(df, .date_col) {
  df %>% 
    # Create time series signature
    mutate(
      date_year        = year({{.date_col}}),
      date_half        = semester({{.date_col}}, with_year = FALSE),
      date_quarter     = quarter({{.date_col}}),
      date_month       = month({{.date_col}}),
      date_month_label = month({{.date_col}}, label = TRUE),
      date_day         = day({{.date_col}}),
      date_wday        = wday({{.date_col}}, week_start = 1),
      date_wday_label  = wday({{.date_col}}, label = TRUE),
      date_qday        = qday({{.date_col}}),
      date_yday        = yday({{.date_col}}),
      date_qid         = str_c(date_quarter,"Q", str_sub(date_year, 3L)) %>% as_factor()
    )
}
```

```{r}
constituents_ts <- constituents %>% 
  add_ts_signature(.date_col = date)
```



```{r}
constituents_ts <- constituents %>% 
  
  # Create time series signature
  mutate(
    date_year        = year(date),
    date_half        = semester(date, with_year = FALSE),
    date_quarter     = quarter(date),
    date_month       = month(date),
    date_month_label = month(date, label = TRUE),
    date_day         = day(date),
    date_wday        = wday(date, week_start = 1),
    date_wday_label  = wday(date, label = TRUE),
    date_qday        = qday(date),
    date_yday        = yday(date),
    date_qid         = str_c(date_quarter,"Q", str_sub(date_year, 3L)) %>% as_factor()
  )

```

How are `actions` distributed along the data?

```{r}
constituents_ts %>% count(action)
```

`current` appears to correspond to the index's most recent constituents. We can check if this is the case by taking the unique values for the `date` variable when the `action` variable has the _current_ value.

```{r}
constituents_ts %>% filter(
  action == "current"
) %>% distinct(date)
```

Indeed, the only date where a _current_ value appears is `r constituents_ts %>% filter(action == "current") %>% distinct(date)`


Let's inspect the data for the first date `1957-03-04`:

```{r}
constituents_wide %>% 
  filter(
    date == ymd("1957-03-04")
  )
```

It's clear that we don't have the initial S&P500 sample, but we **could** reverse engineer the constituents at each date. Let's start by the final date `2020-12-31`:

```{r}
constituents %>% 
  filter(date == ymd("2020-12-31"))
```


We see that we have sort-of-duplicated rows; we note that one `action` label corresponds to `historical` and the other one to `current`. Let's check that we actually have the same number per category:

```{r}
constituents %>% 
  filter(date == ymd("2020-12-31")) %>% 
  count(action)
```

In order to get a better sense of what's going on with the `action` column, we pivot the data to a _wide_ format so we can clearly see the difference between each `action` value. By doing this, we can have a single column for each of the 4 unique values in the `action` variable.

```{r}
constituents_wide <- constituents_ts %>% 
  pivot_wider(
    names_from = action,
    values_from = name
    ) %>% 
  relocate(historical, current, added, removed, .after = ticker)

constituents_wide
```

Inspecting the data, it appears that the `historical` variable gives us the constituents at each quarter. This is backed by the fact that on the latest available date, we have the `current` constituents which correspond to the 4Q20. However, as mentioned above, we have the exact same rows but with a `historical` label.

Let's inspect the dates on which there are no missing values for the `historical` action. We can check the unique values for the month number:

```{r}
constituents_wide %>% 
  filter(!is.na(historical)) %>%
  distinct(date_month, date_quarter)
```


Indeed, the only months were `historical` actions are not missing are _mod 3_ months (i.e. the end of each quarter).

This points to the fact that probably the `historical` value indicates the constituents per quarter on the S&P500. We can further check the amount of `historical` values per quarter. The number should be around 500, of course.

```{r}
constituents_wide %>% 
  select(date_year, date_quarter, historical) %>% 
  filter(date_year > 2000) %>% 
  group_by(date_year, date_quarter) %>%  
  nest() %>% print(n = 80)
```

```{r}
constituents_ts %>% 
  group_by(date_qid) %>%
  arrange(date_year, date_quarter) %>% 
  filter(
    action == "historical",
    date_year > 2009
    ) %>% 
  count(action) %>% 
  arrange(n) %>% 
  print(n = 20)
```

Indeed the number is very close to 500 among all quarters.

One last check we can perform before we can use the `historical` rows as our constituents has to do with the `current` rows. We can reverse engineer the 3Q20 constituents by starting from the 4Q20 (the _current_ constituents).


We can first see what the changes on constituents were between the 3Q20 and 4Q20. We can separate the adds and drops into two data frames, like so:

```{r}
adds_drops <- constituents_wide %>% 
  filter(!is.na(added) | !is.na(removed) , date_qid %in% c("4Q20")) %>% 
  select(date, ticker, added, removed, date_qid)

adds_drops %>% 
  filter(!is.na(added)) %>% 
  select(-removed)

adds_drops %>% 
  filter(!is.na(removed)) %>% 
  select(-added)
```


We can now do the reverse action for the adds and the drops, respectively. Thus, the drops from 3Q20-4Q20 will become our adds and the adds will become our drops.

We start by getting the adds needed to go from 4Q20 to 3Q20:

```{r}
# Extract the adds from 4Q20 to 3Q20
adds_3Q <- adds_drops %>% 
  filter(!is.na(removed)) %>% 
  pull(removed)

adds_3Q
```

Next, we get the drops needed to go from 4Q20 to 3Q20:

```{r}
# Extract the drops from 4Q20 to 3Q20
drops_3Q <- adds_drops %>% 
  filter(!is.na(added)) %>% 
  pull(added)
```


Finally, we get the constituents in 4Q20, include the adds and remove the drops, and compare against the constituents in 3Q20. We should get the same result.

```{r}
# Get all the constituents for 4Q20
constituents_4Q20 <- constituents_wide %>% 
  filter(date_qid == "4Q20", !is.na(historical)) %>% 
  pull(historical) %>% sort()

# Get all the constituents for 3Q20
constituents_3Q20 <- constituents_wide %>% 
  filter(date_qid == "3Q20", !is.na(historical)) %>% 
  pull(historical) %>% sort()

# include the adds from 4Q20
calculated_3Q20 <- c(constituents_4Q20, adds_3Q)

# remove the drops from 4Q20
calculated_3Q20 <- calculated_3Q20[!calculated_3Q20 %in% drops_3Q] %>% sort()

# Check if both results are equal
all.equal(constituents_3Q20, calculated_3Q20)
```

### 2.1.2 The time series of S&P500 constituents
We did get a `TRUE` value when comparing the reverse-engineered approach and the mere `historical` values. We will now proceed to use these `historical` values for every day in the `closes.csv` dataset from `2010-01-01` through `2020-01-01`.


```{r}
# Get the required dates from the closes.csv file
required_dates <- read_csv(file = "data/closes.csv") %>% 
  # Focus on the date only
  select(date) %>% 
  mutate(date_year = year(date)) %>% 
  
  # Focus on 2010 or later
  filter(date_year > 2009)
```

We now nest a data frame for each date in the `constituents` dataset that contains the S&P 500 constituents.

```{r}
constituents_nested <- constituents_wide %>% 
  # Focus on just the historicals and 2010 or later
  filter(date_year > 2009, !is.na(historical)) %>% 
  select(date, ticker, historical, starts_with("date")) %>% 
  nest(constituents = c(ticker, historical)) %>% 
  relocate(constituents, .after = 1) %>% 
  arrange(date)
```

We can now left_join the datasets:

```{r}
required_dates %>% 
  left_join(constituents_nested) %>% 
  fill(constituents, starts_with("date"), .direction = "up")
```

















### 2.2 Data wrangling

```{r}
# Use read_csv() function to read the close data
close_tbl <- read_csv("data/closes.csv") %>% 
  select(-1)

close_tbl
```

Apparently, we have daily closing data from `1997-12-31` through `2021-01-12` for `1,084` different securities.

```{r}
# roc_close <- close_tbl %>% 
#   mutate(
#     across(
#       .cols = -date, 
#       .fns = ~ (.x / lag(.x, n = 200L)) - 1,
#       .names = "ROC_{.col}"
#       )
#     )
```


```{r}
# close_tbl %>% skimr::skim_without_charts()
```


We get the data into a long format (instead of the wide format with `1,085` columns).

```{r}
# close_long <- close_tbl %>% 
#   pivot_longer(cols = -date, names_to = "ticker", values_to = "close") %>% 
#   arrange(ticker, date)
# 
# close_long
```



```{r}
start_date <- ymd("2010-01-01")
end_date <- ymd("2020-01-01")
```



Delivery:

```{r}
tibble(
  date = ymd("2010-01-01"),
  nav = 1e6
)
```

```{r}
tibble(
  date = ymd("2010-01-01"),
  ticker = "AAPL",
  weight = .5
)
```

